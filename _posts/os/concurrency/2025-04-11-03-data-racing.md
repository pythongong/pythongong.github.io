---
layout: post
title:  "Data Racing"
date:   2025-04-11
tag: Operating System
---

Threads run simultaneously, so we have no control over the order in which threads do their work. Similar to the process, the order of execution of different threads is determined by the OS scheduler, and although the scheduler likely implements some sensible algorithm, it is hard to know what will run at any given moment in time. 

Since all threads run in the same address space, they all have access to a global variable. It’s possible for their operations to interleave in such way that global state is not correctly modified. Although such a case may only arise one time out of thousands, it will cause serious problems.
Take a simple C code as example.

```c
int x = 0;
for (int i = 0; i < 4; i++) {
	x += 1;
}
```

 The code `x += 1;` can be translated into the following RSIC-V code

``` bash
lw t0 0(sp)
addi t0 t0 1
sw t0 0(sp)
```

If we run the code `x += 1;` on 4 threads instead of the loop, these instructions can be executed in any order. Although each thread has its own registers, they load the value of x from the same address and store the value of x at the same address. The result will be 1 in this case which should be 4.

``` bash
sw x0 0(sp)
lw t0 0(sp) # T1
addi t0 t0 1 # T1
lw t0 0(sp) # T2
addi t0 t0 1 # T2
sw t0 0(sp) # T2
lw t0 0(sp) # T3
addi t0 t0 1 # T3
sw t0 0(sp) # T3
lw t0 0(sp) # T4
addi t0 t0 1 # T4
sw t0 0(sp) # T4
sw t0 0(sp) # T1
```

The last instruction of the thread 1 is executed at last and other threads are perfectly interleaved. Its order can be explained as the following steps:

1. Thread 1 reads x = 0
2. Thread 2 reads x = 0
3. Thread 2 stores x = 1
4. Thread 3 reads x = 1
5. Thread 3 stores x = 2
6. Thread 4 reads x = 2
7. Thread 4 stores x = 3
8. Thread 1 stores x = 1 as t0 register in the thread 1 still stores 1. 

What we have demonstrated here is called a **race condition** , referred to as a **data race** more specifically, that the results depend on the timing of the code’s execution. Thus, instead of a nice deterministic computation from a single thread program, we call this result **indeterminate**, where it is not known what the output will be and it is indeed likely to be different across runs.

A **critical section** is a piece of code that accesses a shared variable and results in a race condition.  What we really want for this code is what we call mutual exclusion. This property guarantees that if one thread is executing within the critical section, the others will be prevented from doing so.

One way to solve this problem would be to have **atomic instruction** that cannot be interrupted halfway by other threads. Atomic, in this context, means “as a unit”, which sometimes we take as “all or none.” When an interrupt occurs, either the atomic instruction has not run at all, or it has run to completion.

However, in the general case, we won’t have such an instruction. What we will instead do is ask the hardware for a few useful instructions upon which we can build a general set of what we call **synchronization primitives**. By using this hardware support, in combination with some help from the operating system, we will be able to build multi-threaded code that accesses critical sections in a synchronized and controlled manner, and thus reliably produces the correct result. 

You can declare variables as **local variables** within a function  and then pass pointers to those variables as arguments to the new threads. This avoids the global variable and all their attendant risks and gives you direct control and documentation about which routines have access to these pieces of data. The downside is longer argumentation lists for the function and more complicated variable management. But you need to be careful that the stack frame of the function must remain valid for the entire time when its local variables are being used. Since the main function exists for the lifetime of the entire program, we most usually declare variables within the main function.